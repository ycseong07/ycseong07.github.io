---
layout: post
title: CUDA의 역할과 설치 방법
tags: [Ubuntu, CUDA, Docker]
description: pytorch, tensorflow를 사용하기 위해 마련한 서버에 CUDA를 설치하면서 알게된 것들을 정리합니다. 
---
pytorch, tensorflow를 사용하기 위해 마련한 서버에 CUDA를 설치하면서 알게된 것들을 정리합니다. 

### CUDA(Computed Unified Device Architecture)란?
- NVIDIA에서 개발한 GPU 개발 툴
- [Nvidia blog의 소개](https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/)에 따르면, `범용 컴퓨팅을 위해 GPU를 간단하고 우아하게 사용하는 병렬 컴퓨팅
플랫폼 및 프로그래밍 모델`
- CUDA Process는 다음과 같은 흐름에 따라 진행됩니다.
```
1. 메인 메모리를 GPU 메모리로 복사 (따라서 RAM의 용량이 GPU용량 이상이여야 함)
2. CPU가 GPU에 프로세스를 지시
3. GPU의 각 코어에서 병렬처리 수행
4. GPU 메모리로부터의 결과물을 메인 메모리에 복사
```
- 이러한 프로세스를 사용하는 이유는 GPU를 사용해 많은 양의 연산을 병렬처리하기 위함이며,
- CUDA를 사용하려면 Nvidia GPU와 `CUDA Driver`, `CUDA Toolkit`이 필요합니다. 

### CUDA 설치 (Ubuntu 22.04 LTS 기준)
#### CUDA Driver
1. CUDA Toolkit을 설치하면서 CUDA Driver를 함께 설치할 수도 있으니, 특정 드라이버 버전이 필요한 경우에만 아래 내용을 진행합니다.
2. `sudo lshw -c display` 명령어로 현재 사용 중인 그래픽카드 모델과 드라이버를 확인할 수 있습니다.
3. `sudo ubuntu-drivers devices` 명령어로 설치 가능한 Nvidia 그래픽 카드 드라이버 목록을 확인한 후, 
4. `sudo apt install nvidia-driver-[원하는버전]` 명령어로 드라이버를 설치합니다. 
    - `sudo ubuntu-drivers autoinstall` 로 권장 버전을 자동으로 설치할 수도 있습니다.

#### CUDA Toolkit
1. [내 GPU와 호환되는 CUDA Toolkit 버전](https://en.wikipedia.org/wiki/CUDA#GPUs_supported)을 확인합니다. 
    -   만약 GeForce RTX 3080을 사용한다면 compute capability는 8.6이고, 이에 맞는 CUDA SDK 버전(CUDA Toolkit 버전)인 11.1 - 12.0을 사용할 수 있습니다. 2023년 5월 18일 기준 가장 최신 버전인 12.1도 사용할 수 있습니다. 
    - 만약 이전 버전을 사용하고자 하는 경우, 구글에 특정 버전을 검색해 아카이브 페이지에서 다운받을 수 있습니다. (예를 들어, CUDA Toolkit 11.4의 경우 [여기서](https://developer.nvidia.com/cuda-11-4-0-download-archive) 설치할 수 있습니다.)
5. [CUDA Toolkit 다운로드 페이지](https://developer.nvidia.com/cuda-downloads)에 접속한 후, Linux - x86_64 - Ubuntu - 22.04 - runfile(local) 선택 후 출력되는 설치 명령어를 통해 CUDA Toolkit을 설치합니다.
     - 만약 드라이버를 설치하지 않았다면, deb(local)을 선택하면 드라이버까지 함께 설치할 수 있습니다. 

#### cuDNN (CUDA Depp Neural Network) 설치
1. cuDNN은 복잡한 딥러닝 연산을 빠르게 할 수 있도록 도와주는 소프트웨어입니다.
2. [cuDNN 페이지](https://developer.nvidia.com/rdp/cudnn-download)에 접속 후 CUDA 버전에 맞는 cuDNN 다운로드합니다(Nvidia 계정 필요).
3. 다운로드 받은 위치로 이동 후 `tar xvf cudnn` 까지 입력 후 tab을 눌러 전체 압축파일명을 입력해주고, 압축 해제합니다.
4. 아래 명령어로 필요한 파일을 복사합니다
```
sudo cp cudnn-*-archive/include/cudnn*.h /usr/local/cuda/include
sudo cp -P cudnn-*-archive/lib/libcudnn* /usr/local/cuda/lib64
sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*
```
5. `sudo vim ~/.bashrc` 입력 후 맨 하단에
```
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-12.0/lib64/ 
```
을 입력해줍니다(cuda-12.0 부분은 cuda 버전에 따라 변경해줍니다).
6. nvidia-smi 명령어로 CUDA driver 정상작동 확인
7. nvcc -V 명령어로 CUDA runtime 정상작동 확인

#### CUDA driver는 뭐고, CUDA runtime은 뭔가요?
1. CUDA는 driver API와 runtime API를 가지고 있습니다. 
2. driver API는 GPU의 하드웨어 기능에 직접 접근할 수 있는 저수준 인터페이스로, runtime API에 비해 더 복잡하거나 세부적인 세팅이 가능합니다. `GPU driver installer`에 의해 설치되며, `nvidia-smi` 명령어를 통해 버전 확인 가능합니다.
    - 단, 이 경우 출력되는 버전은 현재 설치되어 있는 nvidia driver에서 호환되는 최상위 cuda 버전이 표시됩니다.
3. runtime API는 driver API에 비해 고수준 인터페이스이며, 간단하고 직관적인 방식으로 GPU 커널을 호출하고 관리할 수 있습니다. `CUDA toolkit installer`에 의해 설치되며, `nvcc -V` 명령어를 통해 버전 확인 가능
4. 이 둘의 버전이 다른 것은 이상한게 아니며, 필요에 따라 다르게 설치할 수 있습니다. 

#### CUDA 버전과 호환되는 DL 라이브러리 설치
1. [pytorch 버전 별 설치 방법](https://pytorch.org/get-started/previous-versions/)
2. [tensorflow 버전 별 설치 방법](https://www.tensorflow.org/install/source?hl=ko#gpu)

여기까지 진행해보니, 결국 사용하길 원하는 pytorch 혹은 tensorflow 버전을 먼저 정하고, 그에 맞는 CUDA Toolkit 버전을 선택하는 순서가 맞는 것 같다는 생각을 하게 됐습니다. 만약 여러 버전을 사용해보면서 기능을 비교해보고자 한다면, 혹은 서버 자원을 여러 사람들에게 공유하고자 한다면 여러 Docker Container를 만들어 사용하는게 합리적인 것 같습니다. 

#### Docker Container 내에서 GPU를 사용해보자
1. 만약 로컬(서버)에서 작업하지 않고 docker container 내에서 GPU 를 사용하고 싶다면, 로컬(서버)에 Nvidia Container Toolkit(nvidia-docker)을 설치해줘야 합니다 ([Nvidia Container Toolkit 설치 가이드](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#setting-up-nvidia-container-toolkit)).
2. Nvidia Container Toolkit은 호스트 시스템과 동일한 버전의 Nvidia GPU 드라이버와 CUDA를 컨테이너 내에서 사용할 수 있도록 제공합니다.
3. 이 경우, 로컬(서버)의 CUDA driver API를 그대로 컨테이너 내에 매핑하므로, docker image 생성 시에는 GPU driver installer 설치 명령어는 넣지 않아도 되며, CUDA toolkit 설치 명령어만 포함시키면 됩니다.
4. 즉, 물리적 GPU가 있는 서버에는 CUDA driver와 Nvidia Container Toolkit을, 도커 컨테이너에는 CUDA Toolkit과 cuDNN을 설치해주면 됩니다. 
5. 리마인드) CUDA Toolkit 버전과 호환되는 Python 라이브러리 버전을 사용해야한다는 점에 유의합니다. 
6. Docker Container 내에서 GPU를 사용하려면, Container 실행 시
     - `--gpus all`(모든 GPU 활용), `--gpus 0`(특정 GPU만 활용) 처럼 옵션을 주거나
     - `--runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=all` 처럼 옵션을 줘야합니다.

 <details>
<summary>▶ References</summary>
<div markdown="1">
 - https://actruce.com/en/all-about-the-nvidia-driver-installation/
 - https://stackoverflow.com/questions/53422407/different-cuda-versions-shown-by-nvcc-and-nvidia-smi
 - https://bo-10000.tistory.com/73
 - https://bo-10000.tistory.com/75
 - https://dongle94.github.io/docker/docker-nvidia-docker-install/
</div>
</details>