---
layout: post
title: LLM 개발 입문을 위한 넓고 얕은 사전지식
tags: [NLP, LLM]
description: LLM 개발 프로젝트를 진행하기 위해 빠르게 이전 지식을 복습하고, 이후 프로젝트의 방향에 대한 감을 잡기 위해 빠르게 관련지식을 정리합니다.
---
LLM 모델링을 시작하기 전, LLM 관련 역사와 개념을 빠르게 훑어보고, 향후 진행 방향을 정리해보기 위해 관련 내용을 정리했습니다.

# 목차
1. 용어정리
2. 언어모델의 발전 과정  
    1) RNN (Recurrent Neural Network)  
    2) Seq2Seq(Sequence-to-Sequence)  
    3) 트랜스포머(Transformer)  
    4) BERT (Bidirectional Encoder Representations from Transformers)  
    5) GPT (Generative Pre-trained Transformer)  
    6) LLaMA (Large Language Model Meta AI)
3. LLM 개발 이슈 및 연구 방향


# 1. 용어 정리
- 한 번 가볍게 읽고, 나머지 글을 읽다가 '이게 무슨 뜻이더라?'하는 생각이 들면 다시 돌아와 읽는 것을 추천

## 예시 문장: `자연어 처리 참 쉽죠?`

- 토큰(Token)
    - 문장을 구성하는 최소 단위 요소. 단어, 음절, 형태소 등 기준은 정의하기 나름이지만, 주로 띄어쓰기를 기준으로 단어 단위 토큰화가 많이 쓰임
    - 토큰화(Tokenization)는 텍스트를 토큰으로 분할하는 과정.
    - 단어 단위 토큰화 예시: "자연어", "처리", "참", "쉽죠"
- 시퀀스(Sequence)
    - 토큰을 일렬로 나열한, 순서를 가지는 데이터 구조
    - 자연어 처리에서 문장은 토큰의 시퀀스로 표현
    - 시퀀스 예시: ["자연어", "처리", "참", "쉽죠"]
- 임베딩(Embedding)
    - 단어나 문장을 벡터로 표현하는 기법
    - 과거에, 단어 임베딩 기법으로는 Word2Vec, GloVe 등, 문장 임베딩 기법으로는 Doc2Vec, Skip-Thought Vectors, Universal Sentence Encoder 등이 제안되고 사용됨
    - 문장 임베딩 예시: "자연어 처리 참 쉽죠?" -> [0.1, 0.3, -0.2, 0.4] (실제로 임베딩한 값이 아니며, 예시를 위한 임의의 숫자임)
    - 다만, BERT나 GPT 모델의 경우 위에서 언급된 임베딩 기법이 아닌, 독자적인 임베딩 기법을 사용해 개발됨
    - BERT는 양방향 Transformer 인코더를,GPT 모델의 경우 Transformer 디코더를 사용해 문장 임베딩

# 2. 언어 모델의 발전 과정
- 단어가 모여 문장이 될 때, '말이 되는' 문장이 되려면 단어의 순서가 중요
- 문장을 '순서가 있는 데이터'로 바라보고, 즉, 특정 토큰의 출현 확률은 다른 토큰들의 순서에 영향을 받는다는 가정
- 이에 따라 문장을 처리하는 방식의 개선이 이루어지면서 언어 모델 발전

## 1) RNN (Recurrent Neural Network)

<div align="center" class="image-with-caption">
  <figure>
    <img src="/assets/img/illustration/2023-07-02_1.png" alt="image description">
    <figcaption>RNN 모델 구조</figcaption>
  </figure>
</div>

- 시퀀스 데이터(순서가 있는 데이터)를 처리하기 위해 설계된 신경망 모델
- 이전 시간(t-1)의 출력을 현재 시간(t)의 입력으로 받기 때문에(recurrent), 시간적 의존성을 학습할 수 있음
- 그러나 긴 시퀀스를 처리할 때 이전의 정보를 현재의 상태에 전달하는 과정에서 오래된 정보가 점점 소실되는 문제가 있음(장기 의존성 문제)  
- 장기 의존성 문제를 해결하기 위해 LSTM, GRU 모델 등장 (이 모델들도 RNN 계열)
    - LSTM: 게이트(gate) 구조를 도입해 어떤 정보를 기억하고 잊을 것인지 학습시켜 장기 의존성 문제를 해소
    - GRU: LSTM을 단순화 시킨 구조로 변형
    - 다만, 이들에 대한 설명은 이후에 설명할 언어 모델들과의 접점이 적어 자세히 다루지 않음

 <details>
<summary>▶ 장기 의존성 문제에 대한 조금 자세한 설명</summary>
<div markdown="1">
- 어떤 매커니즘으로 오래된 정보가 점점 손실되는걸까?
- t-1 시점을 기준으로, t 시점의 h(t), 즉 hidden state의 값은 아래와 같이 계산됨  
$$h^{(t)}=\tau\left(Ux^{(t)}+Wh^{(t-1)}\right)$$  
- t-2 시점을 기준으로, t 시점의 h(t)을 계산하는 식을 풀어쓰면 아래와 같이 표현됨  
$$h^{(t)}=\tau\left[Ux^{(t)}+W\tau(Ux^{(t-1)}+Wh^{(t-2)})\right]$$  
- 여기에서, τ[...]의 값은 [-1, 1] 이고, 이 값이 계속 곱해져 0으로 수렴하게 됨
    - feed-forward 시 이후의 시점으로 갈 수록 이전의 정보를 충분히 전달할 수 없고,
    - back-propagation 시 활성화 함수의 기울기가 0에 가깝거나(기울기 소실) 너무 큰 값이 발생할 수 있음(기울기 폭발)

h(t): 시간 t에서 메모리가 담고 있는 지난 시간 단계의 정보(hidden state)  
x(t): 시간 t에서의 입력값  
U: x(t)에 대한 가중치 행렬  
W: h(t-1)에 대한 가중지 행렬  
τ: 활성화 함수 (tanh 혹은 RELU 등)  
</div>
</details>

## 2) Seq2Seq(Sequence-to-Sequence)  

<div align="center" class="image-with-caption">
  <figure>
    <img src="/assets/img/illustration/2023-07-02_2.png" alt="image description">
    <figcaption>Seq2Seq 모델 구조</figcaption>
  </figure>
</div>

- 두 시퀀스를 매핑하는 모델이라는 특징 때문에 문장의 번역, 대화 시스템, 요약 등에 활용되어 왔음
- 인코더, 디코더라는 이름의 두 개의 RNN을 사용하는 모델
    - 인코더는 임의 길의의 입력 시퀀스를 고정 길이 벡터로 변환하고
    - 디코더는 고정 길이 벡터를 다시 출력 시퀀스로 변환
- 그러나 Seq2Seq 모델에는 두 가지 문제가 있음
    - RNN 계열의 모델을 기반으로 하기에 생기는 장기 의존성 문제
    - 고정 길이 벡터에 모든 정보를 압축하면서 생기는 정보 손실 문제
- 기본 RNN 대신 LSTM, GRU를 사용하는 방법으로 장기 의존성 문제는 개선할 수 있었지만, 정보 손실 문제는 해결하지 못함
- Attention Mechanism은 두 가지 문제를 획기적으로 개선한 방법

<div align="center" class="image-with-caption">
  <figure>
    <img src="/assets/img/illustration/2023-07-02_3.png" alt="image description">
    <figcaption>Attention Mechanism</figcaption>
  </figure>
</div>

- Seq2Seq 모델의 인코더에는 여러 Hidden State가 존재하는데, 디코더에는 마지막 Hidden State만을 전달함
- Attention Mechanism은 디코더에서 출력 결과를 예측할 때 인코더의 Hidden State들을 다시 한 번 참고함
- 어느 인코더의 Hidden State를 얼마나 참고할지를 결정하기 위해 어텐션 스코어 (Attention Score)를 계산하고, 이 점수에 따라 각 인코더의 Hidden State에 집중(Attention)함


## 3) 트랜스포머(Transformer)  

<div align="center" class="image-with-caption">
  <figure>
    <img src="/assets/img/illustration/2023-07-02_4.png" alt="image description">
    <figcaption>Transformer 모델 구조</figcaption>
  </figure>
</div>

- Seq2Seq 모델이 RNN을 기반으로 하기에 발생하는 두 가지 문제
    - 병렬 처리 연산이 불가능
    - 입/출력 단어 간 거리가 멀 경우 대응 관계 학습이 잘 되지 않음
- "Attention Is All You Need(2017)" 논문에서 6층의 인코더만 사용한 Transformer 모델이 제안됨
- RNN 없이, Attention만 사용해보자는 아이디어, 근데 이제 그냥 Attention 말고 Self Attention
    - Attention: 입력-출력 간 단어 대응 파악
    - Self Attention: 입/출력 내에서 각 시퀀스 내부의 단어들 대응 파악
- RNN을 없애니 순차적 처리가 필요없어져 병렬 처리가 가능해졌고, 각 단어가 전체 시퀀스와 연결됨 
- Scaled Dot-Product Attention 사용

## 4) BERT (Bidirectional Encoder Representations from Transformers)  

<div align="center" class="image-with-caption">
  <figure>
    <img src="/assets/img/illustration/2023-07-02_5.png" alt="image description">
    <figcaption>BERT 모델 구조</figcaption>
  </figure>
</div>

- Transformer의 양방향 encoder의 구조로 구현된 사전훈련모델
- 기존의 언어모델들이 단방향 학습 방법을 사용한 반면, BERT는 양방향 학습 방법을 사용
- Masked Language Model(MLM)과 Next Sentence Prediction(NSP)를 통해 사전학습
- 트랜스포머 모델에서 인코더 부분만 사용 
- BERT의 초기 버전인 BERT-Base는 BooksCorpus, English Wikipedia, Common Crawl 등 영문 텍스트(2.5TB)로 사전 학습되었고, 약 1억 1,000만 개의 파라미터를 가짐(BERT-Large모델은 약 3억 5,400만 개)
- 문장 분류, 문장 두 개의 관계 분류, 문장 내 단어 레이블링, 질의응답에서 좋은 성능을 보임

## 5) GPT (Generative Pre-trained Transformer)  

<div align="center" class="image-with-caption">
  <figure>
    <img src="/assets/img/illustration/2023-07-02_6.png" alt="image description">
    <figcaption>GPT 모델 구조</figcaption>
  </figure>
</div>

- GPT는 왼쪽에서 오른쪽으로 문맥을 이해하는 단방향 언어 모델로, 하나의 토큰을 입력값으로 받았을 때, 다음에 올 적절한 토큰을 생성하는 모델
- 트랜스포머 모델에서 디코더 부분만 사용(Multi-Head Attention도 사용 x)
- 2018년 개발된 GPT-1모델은 추가적으로 학습하는 과정을 거치지 않는 대신, 모델에게 참고할 만한 예제를 주는 방식으로 문제를 해결하는 것(few-shot learning)을 특징으로 함(BookCorpus: 4.6 GB / 1.3 Billion Tokens)
- GPT-2는 예제가 전혀 제공되지 않고 모델이 주어진 instruction을 기반으로 task를 이해하는 zero-shot learning을 특징으로 하며, 웹페이지 텍스트를 통해 사전 학습됨(WebText (Reddit): 40 GB / 15 Billion Tokens)
- GPT-3, GPT-4 모델에서는 파라미터 수가 기하급수적으로 증가했으며, RLHF(Reinforcement Learning with Human Feedback)을 통한 강화학습으로 성능을 크게 끌어올림 (Common Crawl, WebText2, Books1, Books2 및 Wikipedia: 753 GB)

## LLaMA (Large Language Model Meta AI)
- Meta AI가 “LLaMA: Open and Efficient Foundation Language Models(2023)”라는 제목으로 발표한 논문의 모델
- 학습 시간은 65B 모델 기준 A100 80G 2048장(256노드)로 진행되었으며, 1.4T Token 학습에 21일 소요
- LLaMA는 Transformer를 기반으로 하면서 GPT와 같은 few-shot learning 방식을 사용하지만, 퍼블릭 데이터만 사용해서 훈련하고, 추론 효율성을 높이는 것에 주력함. 
- LLaMA 13B는 GPT-3 175B 모델의 1/10의 크기임에도 성능이 더 뛰어나고, LLaMA 65B모델은 Chinchilla 70B와 PaLM 540B모델과 비슷함

# LLM 개발 이슈 및 연구 방향
- 기반모델(Foundation Model)을 만들기 위해 필요한 비용이 너무 비쌈
    - 이준범님이 개발 중이신 KoLLaMA는 7B 모델이 약 3억원, 13B 모델이 약 6억원 정도 소요
- 따라서 가능하다면 가성비 모델을 고려하는 것이 좋음
    - "Training Compute-Optimal Large Language Model(2022)" 논문에서는 Chinchilla 70B 모델을 예시로, 파라미터의 증가는 학습을 위한 토큰 양의 증가를 수반해야 한다는 점, 데이터 양의 증가는 데이터 질이 담보될 때 유의미하다고 제안
    - "St-moe: Designing stable and transferable sparse expert models(2022)" 논문에서는 Transformer에 drop out 적용, Adafactor optimizer, precision format이 메모리를 효율적으로 사용한다는 점을 착안해올 수 있음
    - [PEFT](https://huggingface.co/blog/peft)는 사전학습된 LLM의 대부분의 파라미터를 고정하고 일부의 파라미터만 파인튜닝 하는 방법. 이를 통해 사전 학습된 모델의 행렬 파라미터를 고정하고, 그것을 low-rank 행렬로 대체하는 LoRa(low-rank adaptation) 적용 ([GPT4All](https://github.com/nomic-ai/gpt4all))
    - 모델의 파라미터를 더 작은 비트 수로 표현하는 Quantization(16bit, 8bit, 4bit) 적용 고려 (성능을 해치지 않는 선에서)

 <details>
<summary>▶ References</summary>
<div markdown="1">
- https://yjjo.tistory.com/17
- https://techblog-history-younghunjo1.tistory.com/496
- https://arxiv.org/pdf/1706.03762.pdf
- https://blog.naver.com/sooftware/221784472231
- https://gaussian37.github.io/dl-concept-transformer/
- https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html
- https://moon-walker.medium.com/compute-budget-%EC%A0%9C%ED%95%9C%EC%9D%84-%EA%B3%A0%EB%A0%A4%ED%95%9C-google%EC%9D%98-%EC%B5%9C%EC%8B%A0-language-model-%EC%97%B0%EA%B5%AC-%EB%8F%99%ED%96%A5-79aaafe3d2c8
- https://youtu.be/Us5ZFp16PaU
- https://www.youtube.com/watch?v=HewtI35-lp8&ab_channel=%ED%95%9C%EA%B5%AD%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%EC%95%84%EC%B9%B4%EB%8D%B0%EB%AF%B8
- https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf
</div>
</details>